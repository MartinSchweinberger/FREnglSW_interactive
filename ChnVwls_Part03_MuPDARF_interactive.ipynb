{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pushing the boundaries - analysing English vowel production among Chinese learners of English - Part 3: Statistical Analysis\n",
                "\n",
                "This is an interactive notebook that reproduces the statistical analysis of the talk *Pushing the boundaries - analysing English vowel production among Chinese learners of English* given July 14, 2023, at the University of Freiburg by Martin Schweinberger.\n",
                "\n",
                "\n",
                "\n",
                "## Preparation\n",
                "\n",
                "\n",
                "load packages\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(tidyverse)\n",
                "library(here)\n",
                "library(adehabitatHR)\n",
                "library(lme4)\n",
                "library(sjPlot)\n",
                "library(report)\n",
                "library(flextable)\n",
                "library(cowplot)      \n",
                "library(randomForest) \n",
                "library(rms)    \n",
                "library(caret) \n",
                "library(Hmisc) \n",
                "library(quanteda)  \n",
                "#library(glmulti) \n",
                "library(partykit)   \n",
                "library(ggparty)\n",
                "library(hunspell)\n",
                "library(janitor)\n",
                "library(viridis)\n",
                "library(TMB)\n",
                "library(MuMIn)\n",
                "# set options\n",
                "options(stringsAsFactors = F)                           \n",
                "options(scipen = 999) \n",
                "options(max.print=10000)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load .rda data\n",
                "cdat  <- base::readRDS(file = here::here(\"data\", \"cleandat.rda\")) %>%\n",
                "  dplyr::filter(vowel != \"ʌ\") %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::relocate(duration, .after = F2) %>%\n",
                "  dplyr::relocate(file, .after = Age)\n",
                "# inspect\n",
                "str(cdat); head(cdat)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "table(cdat$vowel, cdat$word)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reduce data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bdat <- cdat %>%\n",
                "  dplyr::mutate(label = stringr::str_remove_all(label, \":\"),\n",
                "                gender = ifelse(gender == \"f\", \"female\", gender),\n",
                "                gender = ifelse(gender == \"m\", \"male\", gender),\n",
                "                tvariety = ifelse(tvariety == \"us\", \"AmE\", tvariety),\n",
                "                tvariety = ifelse(tvariety == \"gb\", \"BrE\", tvariety)) %>%\n",
                "  droplevels(.)  %>%\n",
                "  dplyr::rename(Vowel = label,\n",
                "                Word = word,\n",
                "                TargetVariety = tvariety,\n",
                "                Gender = gender,\n",
                "                Duration = duration,\n",
                "                Proficiency = prof,\n",
                "                Speaker = speaker) %>%\n",
                "  # clean word\n",
                "  dplyr::mutate(Word = str_remove_all(Word, \"\\\\W\")) %>%\n",
                "  dplyr::filter(hunspell_check(Word) == T) %>%\n",
                "  # remove \"shits\"\n",
                "  dplyr::filter(Word != \"shits\",\n",
                "                Word != \"stat\",\n",
                "                Word != \"whats\")\n",
                "# inspect\n",
                "head(bdat); names(table(bdat$Word))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check frequency of words\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create a vector of words\n",
                "words <- names(table(bdat$Word))\n",
                "# load control corpus (ace, brown, lob files)\n",
                "controlc  <- base::readRDS(file = here::here(\"data\", \"controlc.rda\"))\n",
                "# inspect\n",
                "str(controlc)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "extract word count of control corpus\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cleancontrolc <- controlc %>%\n",
                "  stringr::str_replace_all(\"<.*?>\", \" \") %>%\n",
                "  stringr::str_replace_all(\"[^[:alpha:] ]\", \" \") %>%\n",
                "  stringr::str_squish() %>%\n",
                "  quanteda::tokenize_fastestword() %>%\n",
                "  unlist() %>%\n",
                "  length()\n",
                "# inspect\n",
                "cleancontrolc\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "check how frequent the words are in the control corpus\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "freqs <- sapply(words, function(x){\n",
                "  x <- stringr::str_count(controlc, paste0(\"\\\\W\", x, \"\\\\W\", sep = \"\", collapse = \"\"))\n",
                "})\n",
                "# convert into data frame\n",
                "freqsdf <- data.frame(names(freqs), freqs, cleancontrolc) %>%\n",
                "  dplyr::rename(Word = 1,\n",
                "                all = 3) %>%\n",
                "  dplyr::mutate(Frequency = log(freqs/all*1000)) %>%\n",
                "  dplyr::select(-freqs, -all)\n",
                "# inspect\n",
                "head(freqsdf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Annotate word class\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lexical <- c(\"bad\",  \"bed\", \"best\", \"big\", \"bit\", \"book\", \"books\", \"boost\", \"boots\", \"boss\", \"bought\", \"buds\", \"bus\", \"butts\", \"dad\", \"dead\", \"death\", \"debt\", \"debts\", \"desk\", \"dish\",  \"dust\", \"gap\", \"gas\",  \"good\",  \"guess\", \"head\", \"heads\",  \"hit\", \"hot\", \"key\", \"kid\", \"kids\", \"pass\", \"past\", \"pat\", \"path\", \"pub\", \"pubs\", \"push\", \"sad\", \"said\", \"sat\", \"says\", \"seat\", \"seats\", \"see\", \"seep\", \"sees\", \"set\", \"sets\",  \"shits\", \"shoes\", \"shop\", \"shops\", \"shut\", \"sit\", \"skip\",  \"speak\", \"spots\", \"stat\", \"step\", \"steps\", \"stop\", \"stops\", \"stud\", \"suit\", \"task\", \"tasks\", \"tea\", \"teeth\", \"test\", \"tests\", \"took\", \"top\", \"tough\", \"two\", \"wash\", \"ways\",  \"weak\", \"weed\", \"week\",  \"wish\",  \"wood\")\n",
                "bdat <- bdat %>%\n",
                "  dplyr::mutate(WordClass = ifelse(Word %in% lexical, \"lexical\", \"grammatical\"),\n",
                "                Word = as.vector(Word))\n",
                "bdat <- left_join(bdat, freqsdf, by = \"Word\") %>%\n",
                "  dplyr::mutate(vowel = stringr::str_remove_all(vowel, \"ː\")) \n",
                "# inspect\n",
                "table(bdat$WordClass); head(bdat)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Check durations\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bdat %>%\n",
                "  ggplot(aes(x = vowel, y = Duration)) +\n",
                "  geom_boxplot() \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remove items with exaggerated duration\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nrow(bdat)\n",
                "bdat <- bdat  %>%\n",
                "  # remove rare words\n",
                "  dplyr::group_by(type, Word) %>%\n",
                "  dplyr::mutate(freq = n()) %>%\n",
                "  dplyr::ungroup()\n",
                "# harmonize words\n",
                "nnwords <- bdat %>%\n",
                "  dplyr::filter(type == \"CHN\") %>%\n",
                "  dplyr::group_by(Word) %>%\n",
                "  dplyr::summarise(Freq = n()) %>%\n",
                "  dplyr::pull(Word)\n",
                "\n",
                "# remove rare vowels\n",
                "bdat <- bdat %>%\n",
                "  dplyr::group_by(vowel) %>%\n",
                "  dplyr::mutate(fr = n()) %>%\n",
                "  dplyr::filter(fr > 100) %>%\n",
                "  dplyr::select(-fr) %>%\n",
                "  dplyr::ungroup()\n",
                "# inspect\n",
                "str(bdat); nrow(bdat)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bdat %>%\n",
                "  ggplot(aes(x = vowel, y = Duration)) +\n",
                "  geom_boxplot()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb2 <- bdat %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::mutate(Age = dplyr::case_when(Age < 30 ~ \"18-29\",\n",
                "                                       Age < 40 ~ \"30-39\",\n",
                "                                       Age < 50 ~ \"40-49\",\n",
                "                                       Age > 49 ~ \"50+\",\n",
                "                                       TRUE ~ \"unknown\")) %>%\n",
                "  dplyr::group_by(type, Gender, Age) %>%\n",
                "  dplyr::summarise(speakers = length(table(Speaker))) %>%\n",
                "  tidyr::spread(Age, speakers) %>%\n",
                "  dplyr::ungroup()  %>%\n",
                "  adorn_totals(\"row\")%>%\n",
                "  adorn_totals(\"col\")\n",
                "# save\n",
                "write.table(tb2, here::here(\"tables\", \"tb2_frenglsw.txt\"), sep = \"\\t\")\n",
                "# inspect\n",
                "tb2\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bdat <- bdat %>%\n",
                "  dplyr::mutate(F1 = as.vector(scale(F1)),\n",
                "                F2 = as.vector(scale(F2)),\n",
                "                Duration = as.vector(scale(Duration)),\n",
                "                Age = as.vector(scale(Age)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Split data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nsd <- bdat %>%\n",
                "  dplyr::filter(type == \"ENS\") %>%\n",
                "  dplyr::select(-type, -Proficiency, -Speaker, -file)  %>%\n",
                "  dplyr::mutate(Word = ifelse(Word %in% nnwords, Word, \"other\"))%>%\n",
                "  dplyr::mutate_if(is.character, factor)\n",
                "# inspect\n",
                "head(nsd); str(nsd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remove impossible variables (too many levels)\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nrow(nsd)\n",
                "nsd <- nsd %>%\n",
                "  dplyr::select(-fspeaker)\n",
                "str(nsd); nrow(nsd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split native speaker data into test and training set\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add id to data\n",
                "nsd <- nsd %>% dplyr::mutate(id = 1:nrow(.))\n",
                "# create training set (70%)\n",
                "nsdtrain <- nsd %>% dplyr::sample_frac(0.70)\n",
                "# create test set (30%)\n",
                "nsdtest  <- dplyr::anti_join(nsd, nsdtrain, by = 'id') %>%\n",
                "  dplyr::select(-id)\n",
                "# remove id column\n",
                "nsdtrain <- nsdtrain %>% dplyr::select(-id)\n",
                "# inspect\n",
                "head(nsdtrain); head(nsdtest)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nnsd <- bdat %>%\n",
                "  dplyr::filter(type != \"ENS\") %>%\n",
                "  droplevels() %>%\n",
                "  dplyr::select(-file, -type, -fspeaker)\n",
                "# save predictors associated with proficiency for later\n",
                "pred_nns <- nnsd %>% dplyr::select(Speaker, Proficiency)\n",
                "# remove proficiency variables (for now)\n",
                "nnsd <- nnsd %>%\n",
                "  dplyr::select(-Proficiency, -Speaker)\n",
                "# inspect data\n",
                "head(nnsd); str(nnsd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Harmonize words\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nswords <- nsdtrain %>%\n",
                "  dplyr::group_by(Word) %>%\n",
                "  dplyr::summarise(Freq = n()) %>%\n",
                "  dplyr::pull(Word)\n",
                "nnsd <- nnsd %>%\n",
                "  dplyr::mutate(Word = ifelse(Word %in% nswords, Word, \"other\")) %>%\n",
                "  dplyr::mutate_if(is.character, factor)\n",
                "# inspect\n",
                "str(nnsd); str(nsdtrain); nswords\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Remove superfluous predictors\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# nnsd\n",
                "nnsd <- nnsd %>%\n",
                "  dplyr::select(-id, -Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, \n",
                "                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%\n",
                "  dplyr::rename(Vowel = vowel)\n",
                "# nsdtrain\n",
                "nsdtrain <- nsdtrain %>%\n",
                "  dplyr::select(-Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, \n",
                "                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%\n",
                "  dplyr::rename(Vowel = vowel)\n",
                "# nsdtest\n",
                "nsdtest <- nsdtest %>%\n",
                "  dplyr::select(-Vowel, -TargetVariety, -edist, -barkF1, -barkF2, -lobF1, \n",
                "                -lobF2, -normF1, -normF2, -cF1, -cF2, -ED, -WordType, -freq) %>%\n",
                "  dplyr::rename(Vowel = vowel)\n",
                "# inspect\n",
                "colnames(nnsd); colnames(nsdtrain); colnames(nsdtest)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MuPDARF\n",
                "\n",
                "Prepare data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wrds1 <- names(table(nsdtest$Word))[table(nsdtest$Word) > 0]\n",
                "wrds2 <- names(table(nsdtrain$Word))[table(nsdtrain$Word) > 0]\n",
                "wrds3 <- names(table(nnsd$Word))[table(nnsd$Word) > 0]\n",
                "wrds <- Reduce(intersect, list(wrds1, wrds2, wrds3))\n",
                "# apply to data sets\n",
                "nsdtest <- nsdtest %>%\n",
                "  dplyr::mutate(Word = ifelse(Word %in% wrds, as.character(Word), \"other\"))\n",
                "nsdtrain <- nsdtrain %>%\n",
                "  dplyr::mutate(Word = ifelse(Word %in% wrds, as.character(Word), \"other\"))\n",
                "nnsd <- nnsd %>%\n",
                "  dplyr::mutate(Word = ifelse(Word %in% wrds, as.character(Word), \"other\"))\n",
                "# inspect\n",
                "wrds\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### RF NS\n",
                "\n",
                "Now, we perform a random forest analysis of the native speaker data.\n",
                "\n",
                "\n",
                "Now, we perform a random forest analysis of the native speaker data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set seed\n",
                "set.seed(sum(utf8ToInt(\"RFNS\")))\n",
                "nsrf <- randomForest(Vowel ~ ., data=nsdtrain, ntree=1000, proximity=TRUE, importance=TRUE)\n",
                "# inspect rf results\n",
                "nsrf \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "visualise misclassification\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nmc <- nsrf$confusion[, -7] %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::mutate(Vowel = rownames(.)) %>%\n",
                "  tidyr::gather(NativeChoice, freq, æ:ʊ) %>%\n",
                "  dplyr::mutate_if(is.character, factor) %>%\n",
                "  dplyr::mutate(freq = ifelse(freq == 0, NA, freq))\n",
                "\n",
                "ggplot(nmc, aes(x=Vowel, y=NativeChoice, color=freq, size=freq, label=freq)) +\n",
                "  #geom_tile()+\n",
                "  geom_point() + \n",
                "  geom_text(size = 3, hjust=1.5, color = \"gray20\")+\n",
                "  scale_color_gradient(high=\"darkblue\", low=\"gray90\") +\n",
                "  labs(x = \"Observed vowel (ENS)\", y = \"Predicted vowel\\n(based on ENS trained model)\", title = \"(Mis-)Classification of vowels among ENS\") +\n",
                "  theme_bw()+\n",
                "  theme(legend.position = \"none\",\n",
                "        panel.grid.major = element_blank(), \n",
                "        panel.grid.minor = element_blank())\n",
                "# save\n",
                "ggsave(here::here(\"images\", \"misclass_ns.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we plot the results.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(nsrf)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we plot the out-of-bag error frequencies.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot new precision/error rate\n",
                "oob.error.data <- data.frame(\n",
                "  Trees = rep(1:nrow(nsrf$err.rate), times=ncol(nsrf$err.rate)),\n",
                "  Type = rep(dimnames(nsrf$err.rate)[[2]], each=nrow(nsrf$err.rate)),\n",
                "  Error = as.vector(unlist(nsrf$err.rate)))\n",
                "# visualise\n",
                "ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +\n",
                "  geom_line(aes(color=Type, linetype = Type)) +\n",
                "  theme_bw()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "oob.error.data %>%\n",
                "  dplyr::filter(Type != \"OOB\") %>%\n",
                "  ggplot(aes(x=reorder(Type, -Error, mean), y= Error,  group = Type)) +    \n",
                "  geom_boxplot(fill = \"lightgray\") +\n",
                "  coord_cartesian(ylim = c(0, 1)) +              \n",
                "  theme_bw(base_size = 10) +         \n",
                "  theme(axis.text.x = element_text(size=10),  \n",
                "        axis.text.y = element_text(size=10, face=\"plain\")) + \n",
                "  labs(x = \"\", y = \"Error rate (%)\") + \n",
                "  scale_y_continuous(limits = c(0, 1),\n",
                "                     labels = seq(0, 100, 20),\n",
                "                     breaks = seq(0, 1, .2)) +\n",
                "  scale_color_manual(guide = FALSE)\n",
                "# save\n",
                "ggsave(here::here(\"images\", \"rf_ns.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we check the error rates and accuracy and also check how much the model performs better than a base-line model.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# determine accuracy by prediction\n",
                "# prediction\n",
                "pnsrf <- predict(nsrf, nsdtest)\n",
                "# create confusion matrix\n",
                "confusionMatrix(pnsrf, nsdtest$Vowel)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cmnsd <- confusionMatrix(pnsrf, nsdtest$Vowel)\n",
                "# calculate increase in prediction accuracy compared to base-line model\n",
                "cmnsd$overall[1]\n",
                "cmnsd$overall[5]\n",
                "\n",
                "cmnsd$overall[1]/cmnsd$overall[5]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we inspect which variables are important for the predictions.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "impdat <- data.frame(\n",
                "  Measure = c(rep(\"Accuracy\", length(nsrf$importance[,\"MeanDecreaseAccuracy\"])),\n",
                "              rep(\"Gini\", length(nsrf$importance[,\"MeanDecreaseGini\"]))),\n",
                "  Label = rep(dimnames(nsrf$importance)[[1]], 2),\n",
                "  Value = c(nsrf$importance[,\"MeanDecreaseAccuracy\"], nsrf$importance[,\"MeanDecreaseGini\"]))\n",
                "# ordering\n",
                "impdat <- impdat %>%\n",
                "  dplyr::group_by(Measure) %>%\n",
                "  dplyr::mutate(NormMeasure = scale(Value))\n",
                "# inspect\n",
                "impdat\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "impdat %>%\n",
                "  ggplot(aes(x = reorder(Label, NormMeasure), y = Value)) +\n",
                "  geom_point() +\n",
                "  facet_grid(~Measure, scales=\"free\") +\n",
                "  coord_flip() +\n",
                "  theme_bw() +\n",
                "  labs(x = \"\", y = \"\", title = \"Importance of Predictors in Random Forest\\n (measured as mean decrease if perdictor is absent)\")\n",
                "ggsave(here::here(\"images\", \"VarImpRFnsd.png\"), width = 6, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_nsd <- nsdtest %>%\n",
                "  dplyr::mutate(Prediction = predict(nsrf, nsdtest),\n",
                "                Error = ifelse(Vowel == Prediction, 0, 1)) %>%\n",
                "  dplyr::group_by(Vowel) %>%\n",
                "  dplyr::summarise(all = n(),\n",
                "                   errors = sum(Error),\n",
                "                   Percent = round(errors/all*100, 1)) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "    dplyr::mutate(Vowel = as.character(Vowel),\n",
                "                  Type = \"ENS\") %>%\n",
                "  dplyr::select(-all, -errors)\n",
                "# inspect\n",
                "head(errors_nsd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### RF NNS\n",
                "\n",
                "Now, we use the random forest analysis of the native speakers to predict what vowel a native speaker would have produced in the  non-native speaker contexts In a first step, we extract only non-native speaker data.\n",
                "\n",
                "Next, we use the random forest analysis of the native speakers to predict what vowel a native speaker would have used.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# extract prediction for training data\n",
                "pnns <- predict(nsrf, nnsd) \n",
                "# inspect predictions\n",
                "head(pnns); head(nnsd$Vowel)  \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we create a confusion matrix to check the accuracy of the prediction\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "confusionMatrix(pnns, nnsd$Vowel)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cmnsd <- confusionMatrix(pnns, nnsd$Vowel)\n",
                "# calculate increase in prediction accuracy compared to base-line model\n",
                "cmnsd$overall[1]\n",
                "cmnsd$overall[5]\n",
                "\n",
                "cmnsd$overall[1]/cmnsd$overall[5]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The prediction accuracy increases by `r round(cmnsd$overall[1]/cmnsd$overall[5]*100, 1)` percent if use use our model compared to a no information model.\n",
                "\n",
                "\n",
                "Next, we add the difference between predictions and observed amplification to the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add native choice prediction to data\n",
                "nnsd <- nnsd %>%\n",
                "  dplyr::mutate(NativeChoice = as.vector(pnns),\n",
                "                NativeChoice = as.factor(NativeChoice)) %>%\n",
                "  # code if choice of nns is nativelike or not\n",
                "  dplyr::mutate(Vowel = as.character(Vowel),\n",
                "                NativeChoice = as.character(NativeChoice),\n",
                "                NonNativeLike = ifelse(Vowel == NativeChoice, 0, 1))\n",
                "\n",
                "# inspect new data\n",
                "head(nnsd)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inspect words where the vowels are wrong\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_words <- nnsd %>%\n",
                "  dplyr::filter(NonNativeLike == 1) %>%\n",
                "  dplyr::select(Word, Vowel, NativeChoice) %>%\n",
                "  dplyr::group_by(Word, Vowel, NativeChoice) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::arrange(-freq)\n",
                "# inspect\n",
                "errors_words\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "table(errors_words$Word, errors_words$NativeChoice)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "visualise misclassification\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pmc <- nnsd %>%\n",
                "  dplyr::select(Vowel, NativeChoice) %>%\n",
                "  dplyr::mutate_if(is.character, factor) %>%\n",
                "  dplyr::group_by(Vowel, NativeChoice) %>%\n",
                "  dplyr::summarise(freq = n())\n",
                "\n",
                "ggplot(pmc, aes(x=Vowel, y=NativeChoice, color=freq, size=freq, label=freq)) +\n",
                "  #geom_tile()+\n",
                "  geom_point() + \n",
                "  geom_text(size = 3, hjust=1.5, color = \"gray20\")+\n",
                "  scale_color_gradient(high=\"darkblue\", low=\"gray90\") +\n",
                "  labs(x = \"Observed vowel (CHN)\", y = \"Predicted vowel\\n(based on ENS trained model)\", title = \"(Mis-)Classification of vowels among CHN\") +\n",
                "  theme_bw()+\n",
                "  theme(legend.position = \"none\",\n",
                "        panel.grid.major = element_blank(), \n",
                "        panel.grid.minor = element_blank())\n",
                "# save\n",
                "ggsave(here::here(\"images\", \"misclass_nns.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "errors_nnsd <- nnsd %>%\n",
                "  dplyr::group_by(Vowel, NonNativeLike) %>%\n",
                "  dplyr::summarise(freq = n()) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::group_by(Vowel) %>%\n",
                "  dplyr::summarise(all = sum(freq),\n",
                "                   Percent = round(freq/all*100, 1),\n",
                "                   NonNativeLike = NonNativeLike) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::filter(NonNativeLike == 1) %>%\n",
                "  dplyr::select(-all, -NonNativeLike) %>%\n",
                "  dplyr::mutate(Type = \"CHN\")\n",
                "# inspect\n",
                "head(errors_nnsd); errors_nsd\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dplyr::full_join(errors_nnsd, errors_nsd) %>%\n",
                "  dplyr::mutate_if(is.character, factor) %>%\n",
                "  dplyr::group_by(Vowel) %>%\n",
                "  dplyr::arrange(Vowel) %>%\n",
                "  dplyr::mutate(odr = ifelse(Type == \"ENS\", Percent, NA)) %>%\n",
                "  tidyr::fill(odr, .direction = \"updown\") %>%\n",
                "  dplyr::arrange(-odr) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  ggplot(aes(x = reorder(Vowel, -odr), y = Percent, label = Percent, fill = Type, group = Type)) +\n",
                "  geom_bar(stat=\"identity\", position = position_dodge()) +\n",
                "  geom_text(aes(y = Percent+3), position = position_dodge(0.9), size = 2.5, color = \"grey10\") +\n",
                "  theme_bw() +\n",
                "  labs(x = \"\", y = \"Error rate (%)\") +\n",
                "  scale_fill_manual(values = c(\"gray50\", \"gray80\"),\n",
                "                    labels = c(\"L1 English speakers (ENS, test data)\", \"Chinese learners (CHN)\")) +\n",
                "  theme(legend.position = \"top\",\n",
                "        legend.title = element_text(\"\")) +\n",
                "  coord_cartesian(ylim = c(0, 100))\n",
                "# save\n",
                "ggsave(here::here(\"images\", \"vowelerrors_nns.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GLMM\n",
                "\n",
                "\n",
                "Now, we perform a regression analysis on then difference between native speakers and non-native speakers. We begin by creating fixed-effects intercept-only base-line models.\n",
                "\n",
                "prepare data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# add proficiency variables\n",
                "rdat <- cbind(nnsd, pred_nns) %>%\n",
                "  dplyr::mutate(Proficiency = factor(Proficiency),\n",
                "                NonNativeLike = factor(NonNativeLike)) %>%\n",
                "  dplyr::group_by(Word) %>%\n",
                "  dplyr::mutate(freq = n()) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::mutate(Word = ifelse(freq > 12, as.character(Word), \"other\"),\n",
                "                F1 = round(F1, 0),\n",
                "                F2 = round(F2, 0),\n",
                "                Duration = round(Duration, 3)) %>%\n",
                "  dplyr::mutate_if(is.character, factor) %>%\n",
                "  dplyr::select(-freq, -NativeChoice)\n",
                "# inspect\n",
                "head(rdat); str(rdat)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Modeling\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set options\n",
                "options(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n",
                "nnsd.dist <- datadist(rdat)\n",
                "options(datadist = \"nnsd.dist\")\n",
                "# generate initial minimal regression model \n",
                "# baseline model glm\n",
                "m0 = glmer(NonNativeLike ~ (1 | Word) + (1 | Speaker), family = binomial, data = rdat) \n",
                "# inspect results\n",
                "summary(m0)\n",
                "# inspect \n",
                "sjPlot::tab_model(m0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "r.squaredGLMM(m0)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Model fitting\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# wrapper function for linear mixed-models\n",
                "glmer.glmulti <- function(formula, data, random=\"\",...){\n",
                "  glmer(paste(deparse(formula),random), family = binomial,  data=data, control = glmerControl(optimizer=\"bobyqa\"), ...)\n",
                "}\n",
                "# define formular\n",
                "form_glmulti = as.formula(paste(\"NonNativeLike ~  Vowel + Duration +  Gender + Proficiency + WordClass + Frequency\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Extract best 5 models.\n",
                "\n",
                "***\n",
                "\n",
                "> WARNING: DO NOT EXECUTE THE FOLLOWING CODE CHUNK! It requires Java and takes multiple hours! \n",
                "\n",
                "***\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(glmulti)\n",
                "# multi selection for glmer\n",
                "mfit <- glmulti(form_glmulti, random=\"+(1|Word)+(1|Speaker)\", \n",
                "                data = rdat, method = \"h\", fitfunc = glmer.glmulti,  includeobjects = T,\n",
                "                crit = \"aic\", intercept = TRUE, marginality = FALSE, level = 2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After 2000 models:\n",
                "Best model: NonNativeLike~1+Vowel+Frequency+Vowel:Frequency+WordClass:Frequency\n",
                "Crit= 1083.85798375276\n",
                "Mean crit= 1085.21585106472\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# set seed\n",
                "set.seed(sum(utf8ToInt(\"GLMER\")))\n",
                "# generate final model (include main effects)\n",
                "mf <- glmer(NonNativeLike ~ (1 | Word)  + (1 | Speaker) +\n",
                "              Vowel+Frequency+Vowel:Frequency+WordClass:Frequency,\n",
                "            family = binomial, data = rdat)\n",
                "# inspect \n",
                "sjPlot::tab_model(mf)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "r.squaredGLMM(mf)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "check for multicollinearity\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "car::vif(mf)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "check effects\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "p <- plot_model(mf, type = \"re\", sort.est = TRUE, grid = FALSE)\n",
                "p[[1]] + \n",
                "  theme_bw() +\n",
                "  theme(axis.text.y = element_text(size=4))\n",
                "ggsave2(here::here(\"images\", \"re_spk.png\"), width = 4, height = 12)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "p[[2]] + \n",
                "  theme_bw()\n",
                "ggsave2(here::here(\"images\", \"re_wrds.png\"), width = 6, height = 5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sjPlot::plot_model(mf, type = \"pred\", \n",
                "                   terms = c(\"WordClass\", \"Vowel\"),\n",
                "                   se = FALSE,\n",
                "                   ci.lvl = FALSE,\n",
                "                   colors = viridis(6)) +\n",
                "  theme_bw() +\n",
                "  labs(title = \"\", y = \"Predicted percent non-target-like production\",\n",
                "       x = \"Word class\")\n",
                "ggsave2(here::here(\"images\", \"vowel_wc.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sjPlot::plot_model(mf, \n",
                "                   type = \"pred\", \n",
                "                   terms = c(\"Frequency\",\"Vowel\"), \n",
                "                   se = FALSE,\n",
                "                   ci.lvl = FALSE,\n",
                "                   colors = viridis(6)) +\n",
                "  ggplot2::annotate(geom = \"text\", label = \"/æ/\", x = 3.5, y = .93, color = viridis(6)[1], size = 4.5) +\n",
                "  ggplot2::annotate(geom = \"text\", label = \"/ɪ/\", x = 3.5, y = .98, color = viridis(6)[4], size = 4.5) +\n",
                "  ggplot2::annotate(geom = \"text\", label = \"/u/\", x = -9.5, y = 1, color = viridis(6)[5], size = 4.5) +\n",
                "  ggplot2::annotate(geom = \"text\", label = \"/ʊ/\", x = -9.5, y = .18, color = viridis(6)[6], size = 4.5) +\n",
                "  theme_bw()  +\n",
                "  labs(title = \"\", y = \"Predicted percent non-target-like production\",\n",
                "       x = \"Frequency (logged, centered, scaled)\") \n",
                "ggsave2(here::here(\"images\", \"frequency.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "p <- sjPlot::plot_model(mf, \n",
                "                   type = \"pred\", \n",
                "                   terms = c(\"Vowel\"), \n",
                "                   se = FALSE,\n",
                "                   ci.lvl = FALSE)\n",
                "mf_dat <- p$data$predicted %>%\n",
                "  as.data.frame() %>%\n",
                "  dplyr::mutate(Vowel = names(table(rdat$Vowel))) %>%\n",
                "  dplyr::rename(Prediction = 1) %>%\n",
                "  dplyr::mutate(Prediction = Prediction*100)\n",
                "mf_dat\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mf_dat %>%\n",
                "  ggplot(aes(x = reorder(Vowel, -Prediction, mean), y = Prediction, shape = Vowel, color = Vowel, label = round(Prediction, 3))) +\n",
                "  geom_point(size = 5) +\n",
                "  geom_text(size = 3, hjust=-0.5) +\n",
                "  scale_shape_manual(values = names(table(mf_dat $Vowel))) +\n",
                "  scale_color_manual(values = viridis(6)) +\n",
                "  labs(y =\"Predicted percent non-target-like production\",\n",
                "       x = \"\") +\n",
                "  theme_bw() +\n",
                "  theme(legend.position = \"none\")\n",
                "ggsave2(here::here(\"images\", \"vowel_vwl.png\"), width = 5, height = 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tabulation\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save tables\n",
                "str(rdat)\n",
                "str(nsdtrain)\n",
                "str(nsdtest)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Overview of the data\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb1 <- bdat %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::group_by(type) %>%\n",
                "  dplyr::mutate(speakers = length(table(Speaker))) %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::group_by(type, vowel) %>%\n",
                "  dplyr::summarise(speakers = speakers,\n",
                "                   obs = n()) %>%\n",
                "  unique() %>%\n",
                "  tidyr::spread(vowel, obs) %>%\n",
                "  dplyr::ungroup()  %>%\n",
                "  adorn_totals(\"row\")%>%\n",
                "  adorn_totals(\"col\") %>%\n",
                "  dplyr::mutate(Total = Total-speakers)\n",
                "# save\n",
                "write.table(tb1, here::here(\"tables\", \"tb1_frenglsw.txt\"), sep = \"\\t\")\n",
                "# inspect\n",
                "tb1\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "tabulate proficiency\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb3 <- bdat %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::filter(type == \"CHN\")%>%\n",
                "  dplyr::group_by(Proficiency, Gender) %>%\n",
                "  dplyr::summarise(speakers = length(table(Speaker))) %>%\n",
                "  tidyr::spread(Proficiency, speakers) %>%\n",
                "  dplyr::ungroup()  %>%\n",
                "  adorn_totals(\"row\")%>%\n",
                "  adorn_totals(\"col\")\n",
                "# save\n",
                "write.table(tb3, here::here(\"tables\", \"tb3_frenglsw.txt\"), sep = \"\\t\")\n",
                "# inspect\n",
                "tb3\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "tabulate gender and type\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb4 <- bdat %>%\n",
                "  dplyr::ungroup() %>%\n",
                "  dplyr::group_by(Gender, type) %>%\n",
                "  dplyr::summarise(speakers = length(table(Speaker))) %>%\n",
                "  tidyr::spread(Gender, speakers) %>%\n",
                "  dplyr::ungroup()  %>%\n",
                "  adorn_totals(\"row\")%>%\n",
                "  adorn_totals(\"col\")\n",
                "# save\n",
                "write.table(tb4, here::here(\"tables\", \"tb4_frenglsw.txt\"), sep = \"\\t\")\n",
                "# inspect\n",
                "tb4\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary(bdat)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary(nnsd)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "summary(rdat)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Citation & Session Info\n",
                "\n",
                "Schweinberger, Martin. 2023. Pushing the boundaries - analysing English vowel production among Chinese learners of English - Part 3: Statistical Analysis. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sessionInfo()\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
